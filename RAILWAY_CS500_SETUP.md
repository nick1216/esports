# ğŸ® CS500 Scraper on Railway - Complete Guide

Two ways to run CS500 scraping on Railway: **Button/Endpoint** vs **Cron Job**

---

## ğŸ¯ Option 1: Button/Endpoint (Recommended to Start)

### âœ… What I Just Built

The CS500 endpoint now uses **background tasks** to avoid Railway timeouts!

**How it works:**

1. Click "ğŸ® CS500 Full Scrape" button
2. Endpoint returns immediately (no timeout!)
3. Scraping runs in background (1-2 minutes)
4. Page auto-refreshes after 2 minutes to show new data

### âœ… Pros

- âœ… Easy to test (just click button)
- âœ… Works on Railway (no timeout issues)
- âœ… Good for manual/on-demand scraping
- âœ… Already implemented!

### âš ï¸ Cons

- âš ï¸ Requires manual trigger
- âš ï¸ Not automatic

### ğŸ“‹ Setup (Easy!)

1. **Deploy to Railway** (already done if you followed previous steps)
2. **Add environment variables**:
   ```
   DATABASE_PATH=/data/esports_betting.db
   PROXY_SERVER=http://216.26.250.98:3129
   ```
3. **Create volume**: Mount path `/data`
4. **Test**:
   - Go to your Railway URL
   - Click "ğŸ® CS500 Full Scrape"
   - Wait ~2 minutes
   - Page auto-refreshes with new data

### ğŸ“Š Monitoring

Check Railway logs for:

```
ğŸŒ Using proxy: http://216.26.250.98:3129
ğŸ® Starting full CS500 scrape (match IDs + markets)
Step 1: Collecting match IDs...
âœ… Collected 15 match IDs
Step 2: Scraping markets...
âœ… Scraped 120 markets
```

---

## ğŸ¤– Option 2: Railway Cron Job (For Automation)

If you want **automatic** scraping on a schedule.

### âœ… Pros

- âœ… Fully automatic
- âœ… Runs on schedule (every 5 mins, hourly, etc.)
- âœ… No manual intervention needed
- âœ… No timeout limits

### âš ï¸ Cons

- âš ï¸ More complex setup
- âš ï¸ Costs Railway credits (even on free tier)
- âš ï¸ Requires separate service

### ğŸ“‹ Setup (Advanced)

#### Method A: HTTP Cron (Easier)

1. **In Railway Dashboard**:

   - Click "New" â†’ "Cron Job"
   - Schedule: `*/5 * * * *` (every 5 minutes)
   - Command: Leave blank
   - HTTP Target: `https://your-app.up.railway.app/api/scrape/cs500-full`
   - Method: `POST`

2. **Done!** Railway will hit your endpoint every 5 minutes

#### Method B: Command Cron (More Control)

1. **Create a simple runner script** (`cron_cs500.py`):

   ```python
   import asyncio
   import requests
   import os

   async def main():
       url = os.getenv("APP_URL", "http://localhost:8000")
       response = requests.post(f"{url}/api/scrape/cs500-full")
       print(f"Status: {response.status_code}")
       print(f"Response: {response.json()}")

   if __name__ == "__main__":
       asyncio.run(main())
   ```

2. **In Railway**:
   - Create new Cron Job
   - Schedule: `*/5 * * * *`
   - Command: `python cron_cs500.py`
   - Add environment variable: `APP_URL=https://your-app.up.railway.app`

---

## ğŸ¯ Which Should You Use?

### Use **Button/Endpoint** (Option 1) if:

- âœ… You want to test first
- âœ… You scrape manually/on-demand
- âœ… You want simpler setup
- âœ… You're just starting out

**â†’ This is already implemented and ready to go!**

### Use **Cron Job** (Option 2) if:

- âœ… You want fully automatic scraping
- âœ… You need it to run 24/7 without manual intervention
- âœ… You don't mind the extra setup
- âœ… You're ready for production

---

## ğŸ’¡ Recommended Workflow

### Phase 1: Testing (Use Button)

1. Deploy with button/endpoint
2. Test proxy works
3. Verify data is being scraped
4. Monitor for issues

### Phase 2: Production (Add Cron if needed)

1. Once testing is successful
2. Add Railway Cron Job for automation
3. Keep button for manual override
4. Monitor both systems

---

## ğŸ§ª Testing Your Deployment

### Step 1: Test Endpoint Locally

```powershell
# Set proxy
$env:PROXY_SERVER="http://216.26.250.98:3129"

# Start server
python main.py

# Test endpoint
curl -X POST http://localhost:8000/api/scrape/cs500-full
```

### Step 2: Test on Railway

1. **Via UI**:

   - Go to `https://your-app.up.railway.app`
   - Click "ğŸ® CS500 Full Scrape"
   - Check logs in Railway dashboard

2. **Via API**:
   ```bash
   curl -X POST https://your-app.up.railway.app/api/scrape/cs500-full
   ```

### Step 3: Check Results

**Expected Response:**

```json
{
  "status": "success",
  "message": "CS500 scraping started in background. Check logs for progress.",
  "note": "This may take 1-2 minutes. Refresh the page to see updated data."
}
```

**Check Railway Logs:**

```
ğŸŒ Using proxy: http://216.26.250.98:3129
ğŸŒ Browser configured with proxy: http://216.26.250.98:3129
ğŸ® Starting full CS500 scrape (match IDs + markets)
Step 1: Collecting match IDs...
âœ… Collected 15 match IDs
Step 2: Scraping markets...
âœ… Scraped 120 markets
```

---

## ğŸ› Troubleshooting

### "Background task not completing"

- Check Railway logs for errors
- Verify Xvfb is running
- Check if proxy is working

### "No data after 2 minutes"

- Check logs for scraping errors
- Proxy might be blocked
- CS500 might be down
- Try a different proxy

### "Railway timeout error"

- Should NOT happen with background tasks
- If it does, endpoint returns too early
- Check Railway plan limits

### "Proxy not working"

1. Verify `PROXY_SERVER` environment variable is set
2. Check proxy is online: `curl --proxy http://216.26.250.98:3129 https://ipinfo.io`
3. Look for proxy logs: `ğŸŒ Using proxy:`
4. Try a different proxy

---

## ğŸ“Š Current Setup Summary

### What You Have Now:

- âœ… **3 CS500 endpoints** (full, match IDs only, markets only)
- âœ… **Background task processing** (no timeouts)
- âœ… **UI button** (easy testing)
- âœ… **Auto-refresh** (shows new data after 2 mins)
- âœ… **Proxy support** (routes through Canada)
- âœ… **Database persistence** (Railway volumes)

### Ready to Deploy:

```bash
git add api.py static/script.js
git commit -m "Add background task processing for CS500 scraper"
git push
```

### On Railway:

1. Create volume: `/data`
2. Add variables:
   - `DATABASE_PATH=/data/esports_betting.db`
   - `PROXY_SERVER=http://216.26.250.98:3129`
3. Deploy
4. Test button!

---

## ğŸ‰ Summary

**Answer to your question**:

âœ… **YES, the button will work with Railway!**

I've updated the endpoint to use **background tasks**, so it:

- Returns immediately (no timeout)
- Scrapes in the background
- Auto-refreshes the page after 2 minutes

**Just click the button and check the Railway logs to see progress!** ğŸš€

If you want fully automatic scraping later, you can add a Railway Cron Job, but the button is perfect for testing and manual use.
